\chapter{Building and Testing Jobs}
\section{Overview}

A DUCC job consists of two parts, a JobDriver and one or more
JobProcesses. DUCC jobs are implemented using UIMA-AS. The JobDriver uses the UIMA-AS client
API to send WorkItem CASes to the job's input queue. JobProcesses are deployed
as UIMA-AS services comsuming CASes from the job input queue.

The JD process wraps the job's Collection Reader whose
function is to define the collection of WorkItems to be processed.
The CollectionReader returns a small CAS for each WorkItem containing a
reference to the WorkItem data. The JobDriver then submits each CAS to the job's
input queue so that it can be delivered to the next available JobProcess.

The JobProcess contains the analysis pipeline. A typical job pipeline consists
of the user specified CAS Multiplier (CM), Analysis Engine (AE), CAS
Consumer (CC), along with a built-in DUCC Flow Controller that routes CASes
thru these components. The WorkItem CAS is typically sent only to the CM and returned by
the JobProcess when all child CASes produced by the CM have completed
processing; optionally the CR can configure WorkItem CAS flow to go to the CC 
or to the AE & CC to "flush" context if necessary. 

	\begin{description}
	    \item[Note:] Although the JobDriver will receive back the WorkItem CAS, 
	    there is no provision for any user code to analyze results there. Therefore a
		JobProcess typically adds no results to a WorkItem CAS.
	\end{description}

   \subsection{Basic JobProcess Threading Model}
   In addition to the pipeline definition, CM, AE and CC components, the job
   specification also includes the number of pipeline threads to run in each
   JobProcess. Each pipeline thread receives WorkItems independently.

   DUCC creates an aggreate descriptor for the pipeline, and then creates a
   Deployment Descriptor for the JobProcess which deploys the specified number
   of synchronous pipelines.
   
   \subsection{Alternate Pipeline Threading Model}
   
   Alternately a JobProcess can be fully specified by a user submitted
   Deployment Descriptor. Thus any UIMA-AS service design can be used as a
   JobProcess. In this case the parameter --process_thread_count simply defines
   how many WorkItems CASes will be sent to a JobProcess concurrently.
   
	\begin{description}
	    \item[Note:] A UIMA-AS service may be configured to
	    return child CASes; although child CASes returned from a JobProcess will be
	    ignored by the JobDriver, there may be significant overhead in wasted
	    serialization and I/O.
	\end{description}

\section{Collection Segmentation and Artifact Extraction}

UIMA is built around artifact processing. The standard UIMA pipeline starts with
a Collection Reader that defines collection seqmentation, extracts the artifacts
to be analyzed and initializes the CASes to be delivered to subsequent analytic components. 
A Collection reader designed for a specific data collection is highly reusable
for many different analytic scenarios.

For DUCC collection processing the role of collection segmentation is
implemented by the Collection Reader (CR) running in the JobDriver, while
artifact extraction and CAS initialization implemented in the Cas Multiplier
(CM) running in the JobProcess. The combination of a DUCC Collection Reader and
associated CAS multiplier is also highly reusable. 

\section{CAS Consumer Changes for DUCC}

CAS Consumers in a UIMA pipeline may require changes for scale out into DUCC
jobs, for example to avoid scale out bottlenecks, to preserve collection level
processing, or to flush results at end-of-collection processing.
   
	\begin{description}
	    \item[Federated ouput:] Scaled out DUCC jobs distribute artifact processing
	    to multiple pipeline instances. All instances of a CAS Consumer should have
	    independent access to the output device.
	    \item[Singleton processing:] Collection level processing
	    requiring a singleton process would usually be done as a follow-on job.
	    This avoids introducing a singleton bottleneck, as well as allowing
	    incremental progress so that JobProcess errors due to data-dependent bugs
	    can often be fixed and a repeated run can utilize the incremental progress
	    previously one.
	    \item[Flushing cached data:] In some scenarios each WorkItem delivered to a
	    pipeline would be considered an independent collection. If the CAS Consumer
	    is caching artifact data which needs to be flushed after processing the
	    last artifact, the WorkItem CAS can be routed to the CAS Consumer after
	    the last artifact CAS is processed and used to trigger cache flush.
	\end{description}


\section{Job Development for Existing Pipeline Design}

Assuming that existing job design (CR, CM, CC) is to be reused, job development
is focussed on the Analysis Engine (AE) to be plugged in. Before deploying a new
AE in a multithreaded JobProcess it is wise to run it single threaded
(--process_thread_count 1)to separate basic logic from threading problems.

To debug a JobProcess with eclipse, first create a debug configuration for a
remote java application, specifying Connection Type = Socket Listen on a free
port P. Then add to the job specification the following argument,
--process_debug host:port, where host is the machine running eclipse and port is
the value P used in the debug configuration.

\section{Job Development for New Pipeline Design}


\chapter{Sample Application: Raw Text Processing}
\chapter{Sample Application: Follow-on CAS Processing}
