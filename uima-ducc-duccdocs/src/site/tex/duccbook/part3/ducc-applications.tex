\chapter{Building and Testing Jobs}

\section{Overview}

A DUCC job consists of two process types, a Job Driver process and one or more
Job Processes. These processes are connected together via UIMA-AS.
The Job Driver uses the UIMA-AS client API to send Work Item CASes 
to the job's input queue. Job Processes instantiate the analytic pipeline are deployed
as UIMA-AS services comsuming CASes from the job input queue.

The Job Driver process wraps the job's Collection Reader whose
function is to define the collection of Work Items to be processed.
The CollectionReader returns a small CAS for each Work Item containing a
reference to the Work Item data. The Job Driver then submits each CAS to the job's
input queue so that it can be delivered to an available pipeline in a Job Process.

A typical job's analytic pipeline consists of an Aggregate Analysis Engine comprised by
the user specified CAS Multiplier (CM), Analysis Engine (AE) and CAS
Consumer (CC) components, along with a built-in DUCC Flow Controller.
The Work Item CAS is typically sent only to the CM and returned by
the Job Process when all child CASes produced by the CM have completed
processing; optionally the CR can configure Work Item CAS flow to go to the CC 
or to the AE \& CC to "flush" context if necessary. 

	\begin{description}
	    \item[Note:] Although the Job Driver will receive back the Work Item CAS, 
	    there is no provision for any user code to analyze results there. Therefore a
		Job Process typically adds no results to a Work Item CAS.
	\end{description}

   \subsection{Basic Job Process Threading Model}
   In addition to the pipeline definition of explicitly named CM, AE and CC components, the job
   specification also includes the number of pipeline threads to run in each
   Job Process. Each pipeline thread receives Work Items independently.

   DUCC creates an aggreate descriptor for the pipeline, and then creates a
   Deployment Descriptor for the Job Process which deploys the specified number
   of synchronous pipelines (using the job specification parameter: process\_thread\_count)

   UIMA configuration parameters in the CR, CM, AE or CC components can be overriden using
   job specification parameters: driver\_descriptor\_CR\_overrides, process\_descriptor\_CM\_overrides,
   process\_descriptor\_AE\_overrides and process\_descriptor\_CC\_overrides, respectively.
   
   \subsection{Alternate Pipeline Threading Model}
   
   Alternately a Job Process can be fully specified by a user submitted UIMA-AS
   Deployment Descriptor. Thus any UIMA-AS service deployment can be used as a
   Job Process. In this case the parameter process\_thread\_count just defines
   how many Work Items CASes will be sent to each Job Process concurrently.
   
	\begin{description}
	    \item[Note:] In general a UIMA-AS service may be configured to
	    return child CASes; although child CASes returned from a Job Process will be
	    ignored by the Job Driver, there may be significant overhead in wasted
	    serialization and I/O.
	\end{description}

\section{Collection Segmentation and Artifact Extraction}

UIMA is built around artifact processing. A classic UIMA pipeline starts with
a Collection Reader that defines collection seqmentation, extracts the artifacts
to be analyzed and puts them into the CASes to be delivered to subsequent analytic components. 
A Collection reader designed for a specific data collection is highly reusable
for many different analytic scenarios.

A single Collection Reader supplying artifacts to a large number of analysis pipelines 
would be a bottleneck. Not only would artifact data need to be transported twice across
the compute cluster, but all analysis results would be returned to the Job Driver.
To solve both of these problems, a DUCC Collection Reader (CR) only sends a reference
to the artifacts in the Work Item CAS, and artifact data is read directly by the analysis pipeline.

So, in DUCC collection processing the role of collection segmentation is
implemented by the CR run in the Job Driver, while
artifact extraction and CAS initialization are implemented in the Cas Multiplier
(CM) run in the Job Process. The combination of a DUCC CR and associated CM 
is also highly reusable. 

\begin{description}
    \item[Note:] In many cases it is useful to group multiple artifacts into each
      Work Item CAS. Both DUCC sample applications described below exhibit this design.
\end{description}

\section{CAS Consumer Changes for DUCC}

CAS Consumers in a UIMA pipeline may require changes for scale out into DUCC
jobs, again to avoid scale out bottlenecks, as well as to preserve collection level
processing, or to flush results at end-of-work-item processing.
   
	\begin{description}
	    \item[Federated ouput:] Scaled out DUCC jobs distribute artifact processing
	    to multiple pipeline instances. All instances of a CAS Consumer should have
	    independent access to the output target (filesystem, service, database, etc.).
	    \item[Singleton processing:] Collection level processing
	    requiring that all results go to a singleton process would usually be done as a 
            follow-on job.
	    This avoids introducing a singleton bottleneck, as well as allowing
	    incremental progress so that Job Process errors due to data-dependent analysis bugs
	    can often be fixed, enabling a restarted job to utilize the incremental progress made by
	    previous job runs.
	    \item[Flushing cached data:] In some scenarios each Work Item delivered to a
	    pipeline would be considered an independent collection. If a CAS Consumer
	    caches data which needs to be flushed after processing the
	    last artifact for a Work Item, the Work Item CAS can be routed to the CAS Consumer after
	    the last artifact CAS is processed and used to trigger cache flushing.
	\end{description}


\section{Job Development for an Existing Pipeline Design}

Assuming that an existing job input-output design (CR, CM, CC) is to be reused, job
development is focussed on the Analysis Engine (AE) to be plugged in. Before deploying a new
AE in a multithreaded Job Process it is best to run it single threaded
(process\_thread\_count=1) to separate basic logic errors from threading
problems.

To debug a Job Process with eclipse, first create a debug configuration for a
"remote java application", specifying "Connection Type = Socket Listen" on some
free port P. Start the debug configuration and confirm it is listening on the specified port.
Then, before submitting the job, add to the job specification the argument
process\_debug=host:port, where host is the machine running eclipse and port is
the value P used in the running debug configuration.

When the process\_debug parameter is specified, DUCC will only run a single Job Process
which will connect back to the debug configuration.


\section{Job Development for a New Pipeline Design}

A DUCC job is a UIMA application comprised of user code broken into a Collection
Reader running in the Job Driver and an Agreggate Analysis Engine (analysis pipeline) running in one 
or more Job Processes, with every Job Process running multiple instances of the pipeline, each in a different
thread. The major components of the basic Job Process application are as follows:

\begin{itemize}
  \item User Collection reader - segments the input collection in to Work Items
  \item User CAS Multiplier - inputs a Work Item and segments it into artifacts (CASes)
  \item User Analysis Engine - processes the CASes
  \item User CAS Consumer - outputs results for each Work Item
  \item DUCC built-in Flow Controller - routes Work Item CASes to the CM and optionally to the CC or AE \& CC.
\end{itemize}

\subsection{DUCC built-in Flow Controller}
This flow controller provides separate flows for Work Item CASes and for CASes produced by the CM and/or AE.
Its behavior is controlled by the existence of a CM component, and then further specified by the
org.apache.uima.ducc.Workitem feature structure in the Work Item CAS.

When no CM is defined the Work Item CAS is simply delivered to the AE, and then to the CC if defined. 
Any CASes created by the AE will be routed to the CC.

With a defined CM, the Work Item CAS is delivered only to the CM, and then returned from the JP when processing
of all child CASes created by the CM and AE has completed. Work Item CAS flow can be further refined by the CR by
creating a org.apache.uima.ducc.Workitem feature structure and setting the setSendToLast feature to true,
or by setting the setSendToAll feature to true.

\subsection{Workitem Feature Structure}
This feature structure is defined in DuccJobFlowControlTS.xml, located in uima-ducc-common.jar.
In addition to Work Item CAS flow control features, the WorkItem feature structure includes features that are useful
for a DUCC job application. Here is the complete list of features:

\begin{description}
  \item[sendToLast] (Boolean) - indicates the Work Item CAS be sent to the CC
  \item[sendToAll] (Boolean) - indicates Work Item CAS be sent to the AE and CC
  \item[inputspec] (String) - reference to Work Item input data
  \item[outputspec] (String) - reference to Work Item output data
  \item[encoding] (String) - useful for reading Work Item input data
  \item[language] (String) - used by the CM for setting document text language
  \item[bytelength] (Integer) - size of Work Item
  \item[blockindex] (Integer) - used if a Work Item is one of multiple pieces of an input resource
  \item[blocksize] (Integer) - used to indicate block size for splitting an input resource
  \item[lastBlock] (Boolean) - indicates this is the last block of an input resource
\end{description}

\subsection{Deployment Descriptor (DD) Jobs}
Job Processes with arbitrary aggregate hierarchy, flow control and threading can be fully specified
via a complete UIMA AS Deployment Descriptor. DUCC will modify the input queue to use DUCC's private
broker and input queue name to correspond to the DUCC job ID.

\subsection{Debugging}
It is best to develop and debug the interactions between job application components as one, 
single-threaded UIMA aggregate. DUCC provides an easy way to accomplish this, for both basic
and DD job models, using the all\_in\_one specification parameter.

\begin{description}
    \item[all\_in\_one=local] When set to local, all Job components are run in the same
      single-threaded process, on the same machine as eclipse.
    \item[all\_in\_one=remote] With remote, the single-threaded process is run on a DUCC
      worker machine as a DUCC Managed Reservation. 
\end{description}

To debug an all\_in\_one job with eclipse, first create a debug configuration for a
"remote java application", specifying "Connection Type = Socket Listen" on some
free port P. Start the debug configuration and confirm it is listening on the specified port.
Then, before submitting the all\_in\_one job, add the argument process\_debug=host:port, 
where host is the machine running eclipse and port is
the value P used in the running debug configuration.


\chapter{Sample Application: Raw Text Processing}

\section{Application Function and Design}
This application expects as input a directory containing one or more flat text files, 
uses paragraph boundaries to segment the text into separate artifacts, 
processes each artifact with the OpenNlpTextAnalyzer, and writes
the results as compressed UIMA CASes in zip files. Paragraph boundaries are defined as
two or more consecutive newline characters.

Each input file is a Work Item. In order to facilitate processing scale out, 
an optional blocksize parameter can be specified that will be used to break larger 
files into multiple Work Items. Paragraphs that cross block boundaries are processed
in the block where they started. An error is thrown if a paragraph crosses two block
boundaries. A block with nothing but newline characters is also treated as an error.

An output zip file is created for each Work Item. The CAS compression format is selectable as
compressed XmiCas or UIMA compressed binary form 6 format. When compressed binary
is used, each zip file also contains the full UIMA Type System in compressed text.
CASes in UIMA compressed binary form 6 format have the same flexibility as an XmiCas in that
they can be deserialized into a CAS with a different, but compatible Type System.

By default any previously completed output files found in the output directory are preserved.
During Work Item processing the associated output files have "\_temp" appended to their
filenames, and these incompleted output files are always ignored.

\section{Configuration Parameters}
The Collection Reader for this job is the DuccJobTextCR. It has the following configuration
parameters:

\begin{description}
    \item[InputDirectory] path to directory containing input files.
    \item[OutputDirectory] path to directory for output files.
    \item[IgnorePreviousOutput] (optional) boolean to ignore (overwrite) previous output files.
    \item[Encoding] (optional) character encoding of the input files.
    \item[Language] (optional) language of the input documents, i.e. cas.setDocumentLanguage(language).
    \item[BlockSize] (optional) integer value used to break larger input files into multiple Work Items.
    \item[SendToLast] (optional) boolean to route WorkItem CAS to last pipeline component. Set to true in this application.
    \item[SendToAll] (optional) boolean to route WorkItem CAS to all pipeline components. Not used in this application.
\end{description}

The CAS Consumer is the DuccCasCC and has the following configuration parameters:

\begin{description}
  \item[XmiCompressionLevel] (optional) compression value if using ZIP compression. Default is 7.
  \item[UseBinaryCompression] (optional) boolean to select UIMA binary CAS compression.
\end{description}

\section{Set up a working directory}
For this and the following sample program, create a working directory in a writable filesystem.

Copy to this directory the example job specification files:
\begin{verbatim}
   cp $DUCC_HOME/examples/descriptors/org/apache/uima/ducc/sampleapps/*.job .
\end{verbatim}

Copy a UIMA logger configuration file that supresses tons of output from OpenNLP:
\begin{verbatim}
   cp $DUCC_HOME/examples/descriptors/org/apache/uima/ducc/sampleapps/ConsoleLogger.properties .
\end{verbatim}

Copy the executable code and resources for the DUCC sample application components:
\begin{verbatim}
   cp $DUCC_HOME/examples/lib/uima-ducc-examples.jar .
\end{verbatim}

Note that the source code for DUCC sample applications is in ducc\_runtime/examples/src,
with descriptors in ducc\_runtime/examples/descriptors.

\section{Download and Install OpenNLP}
Download OpenNLP from http://opennlp.apache.org and follow the directions in the
UIMA Integration section of their manual to build the UIMA pear file.
Then {\em install} the UIMA pear file in the working directory and
test it with the UIMA Cas Visual Debugger application.

After testing, a small modification of the installed OpenNLP descriptor file
is necessary for DUCC to run it multithreaded. 
Edit opennlp.uima.OpenNlpTextAnalyzer/desc/OpenNlpTextAnalyzer.xml
and change the setting for {\em multipleDeploymentAllowed} from false to true.

\section{Get some Input Text}
Choose one or more flat text files in UTF8 format that only use newline characters,
{\em not CR-LF sequences}.
The text should be big enough to see the impact of DUCC job scale out.
We used test data from the core UIMA test set,
\begin{verbatim}
   http://svn.apache.org/viewvc/uima/uimaj/trunk/uimaj-core/src/test/resources/data/moby.txt
\end{verbatim}
and downloaded it to a subdirectory `Moby'.

\section{Run the Job}
The job specification, DuccRawTextSpec.job, uses a placeholder to reference the working directory
and various operational components located there. For this example the placeholders will be resolved
from environmental variables. Some of the job components, for example
the Job Driver descriptor, is found via the classpath in uima-ducc-examples.jar.

The job is submitted from the command line with the following:
\begin{verbatim}
   MyAppDir=/pathToWorkingDirectory \
   MyInputDir=$MyAppDir/Moby \
   MyOutputDir=$MyAppDir/Moby.processed \
   pathToDuccRuntime/bin/ducc_submit -f DuccRawTextSpec.job
\end{verbatim}

Given that moby.txt is 1.2Mbytes and a blocksize of 100000 is used, there are 13 Work Items. Each Job Process is configured to run 8 parallel OpenNLP pipelines. For comparison, the job can be forced to run all Work Items in a single process with only one thread:
\begin{verbatim}
   MyAppDir=/pathToWorkingDirectory \
   MyInputDir=$MyAppDir/Moby \
   MyOutputDir=$MyAppDir/Moby.processed \
   pathToDuccRuntime/bin/ducc_submit -f DuccRawTextSpec.job \
   --process_deployments_max 1 \
   --process_thread_count 1
\end{verbatim}

\section{Output Data}
There will be an output zipfile for every Work Item, the zipfile containing a compressed CAS for each document (paragraph) 
found in the Work Item. If UseBinaryCompression=true each zipfile will also contain the TypeSystem for the CASes. 
This is needed when deserializing these CASes into a different TypeSystem.

DuccTextCM finds 768 paragraphs in moby.txt. The total size of the resultant 768 XmiCas files is 108MB. The total size using ZIP compression and packaging these into 13 output Work Item zipfiles is 20MB. Using UIMA binary compressed format the total size is 6.5MB.

This output data will be used as input data for the following CAS input processing sample application.


\chapter{Sample Application: CAS Input Processing}

\section{Application Function and Design}
The main purpose of this application is to demonstrate the overhead of processing a collection of CASes grouped into zipfiles 
and compressed with ZIP or with UIMA compressed binary form 6 CAS format.


\section{Configuration Parameters}
The Collection Reader for this job is the DuccJobCasCR. It has the following configuration
parameters:

\begin{description}
    \item[InputSpec] path to directory containing input files (named InputSpec in the hope that more options will be added).
    \item[OutputDirectory] path to directory for output files.
    \item[IgnorePreviousOutput] (optional) boolean to ignore (overwrite) previous output files.
    \item[SendToLast] (optional) boolean to route WorkItem CAS to last pipeline component. Set to true in this application.
    \item[SendToAll] (optional) boolean to route WorkItem CAS to all pipeline components. Not used in this application.
\end{description}


The CAS Consumer is the DuccCasCC and has the following configuration parameters:

\begin{description}
  \item[XmiCompressionLevel] (optional) compression value if using ZIP compression. Default is 7.
  \item[UseBinaryCompression] (optional) boolean to select UIMA binary CAS compression.
\end{description}

\section{Run the Job}
The job specification, DuccCasInputSpec.job, uses a placeholder to reference the working directory
and various operational components located there. For this example the placeholders will be resolved
from environmental variables. Some of the job components, for example
the Job Driver descriptor, is found via the classpath in uima-ducc-examples.jar.

The job is submitted from the command line with the following:
\begin{verbatim}
   MyAppDir=/pathToWorkingDirectory \
   MyInputDir=$MyAppDir/Moby.processed \ 
   MyOutputDir=$MyAppDir/Moby.followon \
   pathToDuccRuntime/bin/ducc_submit -f DuccCasInputSpec.job
\end{verbatim}

\section{Limiting Job Resources}
Using UIMA binary compression format for both input and output CASes, a single-threaded Jop Process will complete in about 8 seconds. That translates to roughly 100 CASes per second (average uncompressed XmiCas size of 140KB) reading, deserializing, re-serializing and re-writing compressed CAS data.
Although this thread is primarily CPU bound on serialization work, it would not be hard to become I/O bound with enough threads banging on a shared filesystem. For this sample job, DuccCasInputSpec.job limits the total number of processing threads to 32 using the combination of process\_thread\_count=8 and process\_deployments\_max=4.


